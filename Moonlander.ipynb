{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55c880ad-2d51-4aa0-a358-707641d73f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n",
      "Requirement already satisfied: gymnasium[classic-control] in /opt/conda/lib/python3.11/site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from gymnasium[classic-control]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from gymnasium[classic-control]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.11/site-packages (from gymnasium[classic-control]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.11/site-packages (from gymnasium[classic-control]) (0.0.4)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /opt/conda/lib/python3.11/site-packages (from gymnasium[classic-control]) (2.6.0)\n",
      "Requirement already satisfied: renderlab in /opt/conda/lib/python3.11/site-packages (0.1.20230421184216)\n",
      "Requirement already satisfied: moviepy in /opt/conda/lib/python3.11/site-packages (from renderlab) (1.0.3)\n",
      "Requirement already satisfied: gymnasium in /opt/conda/lib/python3.11/site-packages (from renderlab) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from gymnasium->renderlab) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from gymnasium->renderlab) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.11/site-packages (from gymnasium->renderlab) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.11/site-packages (from gymnasium->renderlab) (0.0.4)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /opt/conda/lib/python3.11/site-packages (from moviepy->renderlab) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /opt/conda/lib/python3.11/site-packages (from moviepy->renderlab) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /opt/conda/lib/python3.11/site-packages (from moviepy->renderlab) (2.32.3)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /opt/conda/lib/python3.11/site-packages (from moviepy->renderlab) (0.1.10)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /opt/conda/lib/python3.11/site-packages (from moviepy->renderlab) (2.34.2)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from moviepy->renderlab) (0.5.1)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.11/site-packages (from imageio<3.0,>=2.5->moviepy->renderlab) (10.4.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from imageio-ffmpeg>=0.2.0->moviepy->renderlab) (70.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (2024.6.2)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.11/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.11/site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: gymnasium[box2d] in /opt/conda/lib/python3.11/site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from gymnasium[box2d]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from gymnasium[box2d]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.11/site-packages (from gymnasium[box2d]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.11/site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /opt/conda/lib/python3.11/site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /opt/conda/lib/python3.11/site-packages (from gymnasium[box2d]) (2.6.0)\n",
      "Requirement already satisfied: swig==4.* in /opt/conda/lib/python3.11/site-packages (from gymnasium[box2d]) (4.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gymnasium[classic-control]\n",
    "!pip3 install renderlab\n",
    "!pip3 install opencv-python\n",
    "!pip install gymnasium[box2d]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "290c0897-e764-490a-896c-37139c673b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "import renderlab as rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e582380e-94d5-4e9e-b8e8-6e572c4acbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zerofactory():\n",
    "    return np.zeros(env.action_space.n)\n",
    "\n",
    "class MoonlanderAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.95,\n",
    "    ):\n",
    "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
    "        of state-action values (q_values), a learning rate and an epsilon.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: The learning rate\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay for epsilon\n",
    "            final_epsilon: The final epsilon value\n",
    "            discount_factor: The discount factor for computing the Q-value\n",
    "        \"\"\"\n",
    "        self.q_values = defaultdict(zerofactory)\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = []\n",
    "\n",
    "\n",
    "    \n",
    "    def get_action(self, obs: tuple[int, int, bool]) -> int:\n",
    "        \"\"\"\n",
    "        Returns the best action with probability (1 - epsilon)\n",
    "        otherwise a random action with probability epsilon to ensure exploration.\n",
    "        \"\"\"\n",
    "        # with probability epsilon return a random action to explore the environment\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "\n",
    "        # with probability (1 - epsilon) act greedily (exploit)\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        obs: tuple[int, int, bool],\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_obs: tuple[int, int, bool],\n",
    "    ):\n",
    "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "        temporal_difference = (\n",
    "            reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
    "        )\n",
    "\n",
    "        self.q_values[obs][action] = (\n",
    "            self.q_values[obs][action] + self.lr * temporal_difference\n",
    "        )\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - epsilon_decay)\n",
    "\n",
    "def load(path):\n",
    "    with open(path, \"br\") as f:\n",
    "        return pickle.load(f)        \n",
    "\n",
    "def discretize(obs, decimal):\n",
    "    return np.round(obs, decimals = decimal)\n",
    "\n",
    "def save(obj, path):\n",
    "    with open(path, \"bw\") as f:\n",
    "        pickle.dump(obj, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "760b48ef-6b0f-45e6-8fba-aa9708460640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███▍                               | 9999/100000 [14:51<2:17:01, 10.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward over the last 10000: -163.83796710338592\n",
      "Max reward over the last 10000: 105.265380859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████▊                           | 19999/100000 [33:07<2:25:26,  9.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward over the last 10000: -159.49313065757752\n",
      "Max reward over the last 10000: 124.91085815429688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██████████▏                       | 30000/100000 [55:54<4:05:54,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward over the last 10000: -174.69609139289855\n",
      "Max reward over the last 10000: 119.8389892578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████▊                   | 40000/100000 [1:24:09<2:25:56,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward over the last 10000: -185.63490905685424\n",
      "Max reward over the last 10000: 110.0551528930664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████▊                   | 40000/100000 [1:27:39<2:11:29,  7.60it/s]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage reward over the last \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39maverage(rewards)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax reward over the last \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmax(rewards)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m         \u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./checkpoint-Moonlander\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m save(agent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./checkpoint-Moonlander\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m env \u001b[38;5;241m=\u001b[39m rl\u001b[38;5;241m.\u001b[39mRenderFrame(env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./out\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 80\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, path)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(obj, path):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 80\u001b[0m         \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\n",
    "    \"LunarLander-v2\",\n",
    "    continuous = False,\n",
    "    gravity = -10.0,\n",
    "    enable_wind = False,\n",
    "    wind_power = 15.0,\n",
    "    turbulence_power = 1.5,render_mode=\"rgb_array\"\n",
    ")\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_episodes = 100000\n",
    "start_epsilon = 1.0\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)  # reduce the exploration over time\n",
    "final_epsilon = 0.1\n",
    "num_chunks = 10\n",
    "chunk_length = int(n_episodes/num_chunks)\n",
    "rewards = np.zeros(chunk_length)\n",
    "\n",
    "\n",
    "agent = MoonlanderAgent(\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")\n",
    "#agent = load(\"./checkpoint-Moonlander\")\n",
    "\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    obs = discretize(obs, 1)\n",
    "    done = False\n",
    "\n",
    "    # play one episode\n",
    "    while not done:\n",
    "        action = agent.get_action(tuple(obs))\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        next_obs = discretize(next_obs, 1)\n",
    "        # update the agent\n",
    "        agent.update(tuple(obs), action, reward, terminated, tuple(next_obs))\n",
    "\n",
    "        # update if the environment is done and the current obs\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "    agent.decay_epsilon()\n",
    "    rewards[episode%chunk_length] = info[\"episode\"][\"r\"]\n",
    "    if episode%chunk_length == 0 and episode > 0:\n",
    "        print(f\"average reward over the last {chunk_length}: {np.average(rewards)}\")\n",
    "        print(f\"Max reward over the last {chunk_length}: {np.max(rewards)}\")\n",
    "        save(agent, \"./checkpoint-Moonlander\")\n",
    "\n",
    "\n",
    "save(agent, \"./checkpoint-Moonlander\")\n",
    "env = rl.RenderFrame(env, \"./out\")\n",
    "\n",
    "for _ in range(5):\n",
    "    done = False\n",
    "    \n",
    "    obs, info = env.reset()\n",
    "    obs = discretize(obs, 1)\n",
    "    while not done:\n",
    "        action = agent.get_action(tuple(obs))\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        next_obs = discretize(next_obs, 1)\n",
    "        # update if the environment is done and the current obs\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc3cff5-f243-46cc-a30d-599e9ac3d40b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01efca5b-c068-4500-af0c-325e503abe21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
